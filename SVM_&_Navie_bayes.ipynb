{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#SVM & Navie bayes"
      ],
      "metadata": {
        "id": "ItPCjPz0Lgrv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Support Vector Machine (SVM)?\n",
        " - A Support Vector Machine (SVM) is a powerful supervised learning algorithm used for classification and regression tasks. It works by finding the optimal hyperplane that separates different classes in a high-dimensional space. The objective is to maximize the margin between the classes to ensure better generalization to new data."
      ],
      "metadata": {
        "id": "28tsofTKLjei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between Hard Margin and Soft Margin SVM?\n",
        "\n",
        " -         - Hard Margin SVM assumes that the data is perfectly separable and finds a strict boundary without allowing misclassification. This can lead to overfitting if data contains noise.\n",
        "\n",
        "          - Soft Margin SVM introduces a flexibility parameter (C) to allow some misclassification, making it better suited for real-world data that may not be perfectly separable."
      ],
      "metadata": {
        "id": "ahGNm7mbMRJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the mathematical intuition behind SVM?\n",
        " - SVM solves a convex optimization problem to find the hyperplane that maximizes the margin between different classes. It relies on Lagrange multipliers and the Karush-Kuhn-Tucker conditions to formulate the dual problem, making it computationally efficient even for high-dimensional data."
      ],
      "metadata": {
        "id": "KynverDKMyV-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the role of Lagrange Multipliers in SVM?\n",
        " - Lagrange multipliers help transform the constrained optimization problem into an unconstrained problem. They play a crucial role in ensuring that only the support vectors (data points on the margin) influence the decision boundary."
      ],
      "metadata": {
        "id": "jlfX4TaAM4s8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are Support Vectors in SVM?\n",
        " - Support vectors are the data points closest to the hyperplane. These points define the decision boundary and play a critical role in determining the margin. They are the only points that influence the classifier‚Äôs performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "sph-KC4jM9wO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is a Support Vector Classifier (SVC)?\n",
        " - A Support Vector Classifier (SVC) is a type of SVM specifically designed for classification problems. It finds the best hyperplane that separates classes using the support vectors and can work with both linear and non-linear decision boundaries."
      ],
      "metadata": {
        "id": "-DNOBTcSNYVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is a Support Vector Regressor (SVR)?\n",
        " - A Support Vector Regressor (SVR) applies the principles of SVM to regression tasks. Instead of finding a classification boundary, it identifies a function that fits the data while minimizing error within a defined margin."
      ],
      "metadata": {
        "id": "ymrcyXwQNcbq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the Kernel Trick in SVM?\n",
        " - The kernel trick allows SVM to transform data into a higher-dimensional space where it becomes linearly separable. Instead of computing the transformation explicitly, kernel functions compute the dot product efficiently in the transformed space."
      ],
      "metadata": {
        "id": "3fnwMcIpNiK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Compare Linear Kernel, Polynomial Kernel, and RBF Kernel.\n",
        "\n",
        " -        - Linear Kernel: Works well for linearly separable data and is computationally efficient.\n",
        "\n",
        "          - Polynomial Kernel: Captures complex relationships but can be computationally expensive for large datasets.\n",
        "\n",
        "          - RBF Kernel: Maps data to infinite-dimensional space, making it highly effective for non-linearly separable data."
      ],
      "metadata": {
        "id": "ahbNMgBnNmFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is the effect of the C parameter in SVM?\n",
        " - The parameter C controls the trade-off between maximizing the margin and minimizing classification error. A high value of C prioritizes correctly classifying all data points but may lead to overfitting."
      ],
      "metadata": {
        "id": "ZEtfzO7vNzBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is the role of the Gamma parameter in RBF Kernel SVM?\n",
        " - Gamma defines how far the influence of a single training example reaches. A low gamma results in a linear boundary, while a high gamma makes the decision boundary complex and more sensitive to individual points."
      ],
      "metadata": {
        "id": "9FA5fiNrN4Ng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"?\n",
        " - The Na√Øve Bayes classifier is a probabilistic algorithm based on Bayes' theorem. It is called \"Na√Øve\" because it assumes that features are independent of each other, which is often not true in real-world scenarios."
      ],
      "metadata": {
        "id": "a3YSwdlZN9aS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is Bayes' Theorem?\n",
        " - Bayes' theorem expresses the probability of a hypothesis given prior knowledge:\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùê¥\n",
        "‚à£\n",
        "ùêµ\n",
        ")\n",
        "=\n",
        "ùëÉ\n",
        "(\n",
        "ùêµ\n",
        "‚à£\n",
        "ùê¥\n",
        ")\n",
        "ùëÉ\n",
        "(\n",
        "ùê¥\n",
        ")\n",
        "ùëÉ\n",
        "(\n",
        "ùêµ\n",
        ")\n",
        "It is widely used in classification problems to update the likelihood of a hypothesis based on new evidence."
      ],
      "metadata": {
        "id": "f_Tt9K5iOCSE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve Bayes, and Bernoulli Na√Øve Bayes.\n",
        "\n",
        " -          - Gaussian Na√Øve Bayes assumes continuous data is normally distributed.\n",
        "\n",
        "            - Multinomial Na√Øve Bayes is used for discrete data like word counts in text classification.\n",
        "\n",
        "            - Bernoulli Na√Øve Bayes works with binary features, determining whether words are present or absent."
      ],
      "metadata": {
        "id": "bvi1it8-OMAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. When should you use Gaussian Na√Øve Bayes over other variants?\n",
        " - Gaussian Na√Øve Bayes is preferred when working with continuous numerical data, such as sensor readings or financial metrics."
      ],
      "metadata": {
        "id": "mOzGHdCTOWsh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What are the key assumptions made by Na√Øve Bayes?\n",
        "\n",
        " -         - Features are independent given the class.\n",
        "\n",
        "            - Prior probabilities can be estimated accurately.\n",
        "\n",
        "            - The conditional probability of each feature can be computed separately."
      ],
      "metadata": {
        "id": "m4zMVx5jOsoz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are the advantages and disadvantages of Na√Øve Bayes?\n",
        " -     - Advantages:\n",
        "\n",
        "                 i) Computationally efficient.\n",
        "\n",
        "                 ii) Works well for text classification.\n",
        "\n",
        "                 iii) Performs well even with small amounts of data.\n",
        "         - Disadvantages:\n",
        "\n",
        "                 i) Assumes feature independence, which is often unrealistic.\n",
        "\n",
        "                 ii) Struggles with complex relationships in data."
      ],
      "metadata": {
        "id": "H-oAR2K4QD0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Why is Na√Øve Bayes a good choice for text classification?\n",
        " - It is effective because words in a document often appear independently. The model efficiently computes probabilities, making it suitable for spam detection and sentiment analysis."
      ],
      "metadata": {
        "id": "A7hTQA39Qss3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Compare SVM and Na√Øve Bayes for classification tasks.\n",
        "\n",
        " -         - SVM: Works well with complex patterns, requires tuning, and is computationally expensive.\n",
        "\n",
        "            - Na√Øve Bayes: Faster and simpler, but may struggle when features are dependent."
      ],
      "metadata": {
        "id": "AhDKtLO6B_4y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How does Laplace Smoothing help in Na√Øve Bayes?\n",
        " - Laplace smoothing prevents probability estimates from becoming zero when certain words or features are missing in the training data. It ensures robustness in predictions."
      ],
      "metadata": {
        "id": "YKo8WxmsCH4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical"
      ],
      "metadata": {
        "id": "9S7EpUCACLtl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy."
      ],
      "metadata": {
        "id": "QFcySQmKCNzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "X, y = datasets.load_iris(return_X_y=True)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred = svm.predict(X_test)\n",
        "print(f\"SVM Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "M-kwedWsFMfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies."
      ],
      "metadata": {
        "id": "SRPXSFcyFNBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = datasets.load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "kernels = ['linear', 'rbf']\n",
        "for kernel in kernels:\n",
        "    svm = SVC(kernel=kernel)\n",
        "    svm.fit(X_train, y_train)\n",
        "    y_pred = svm.predict(X_test)\n",
        "    print(f\"SVM Accuracy with {kernel} kernel: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "9vH1nwPIFQzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "Ay4Xs1aFFRJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate housing-like dataset\n",
        "X, y = make_regression(n_samples=300, n_features=5, noise=0.1, random_state=42)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVR model\n",
        "svr = SVR(kernel='rbf')\n",
        "svr.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred = svr.predict(X_test)\n",
        "print(f\"Mean Squared Error: {mean_squared_error(y_test, y_pred):.2f}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "jddt1zq2FUXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision boundary."
      ],
      "metadata": {
        "id": "hkIx5QxBFUwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "X, y = make_classification(n_samples=200, n_features=2, n_classes=2, random_state=42)\n",
        "\n",
        "svm_poly = SVC(kernel='poly', degree=3, gamma='scale')\n",
        "svm_poly.fit(X, y)\n",
        "\n",
        "xx, yy = np.meshgrid(np.linspace(X[:, 0].min()-1, X[:, 0].max()+1, 100),\n",
        "                     np.linspace(X[:, 1].min()-1, X[:, 1].max()+1, 100))\n",
        "\n",
        "Z = svm_poly.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
        "plt.title(\"SVM with Polynomial Kernel\")\n",
        "plt.show()\n",
        "'''"
      ],
      "metadata": {
        "id": "16nXpWyQFYI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer dataset and evaluate accuracy."
      ],
      "metadata": {
        "id": "VOsFU6wMFYp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "print(f\"Gaussian Na√Øve Bayes Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "YCf837lDFb50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Write a Python program to train a Multinomial Na√Øve Bayes classifier for text classification using the 20 Newsgroups dataset."
      ],
      "metadata": {
        "id": "MLH70p81FcVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_20newsgroups(subset='train', categories=['sci.space', 'comp.graphics'])\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text to numerical features\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Multinomial Na√Øve Bayes\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_vec, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred = nb.predict(X_test_vec)\n",
        "print(f\"Multinomial Na√Øve Bayes Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "-pwXYf0TFg9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Write a Python program to train an SVM Classifier with different C values and compare the decision boundaries visually."
      ],
      "metadata": {
        "id": "XK87tidqFhiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "X, y = datasets.make_classification(n_samples=200, n_features=2, n_classes=2, random_state=42)\n",
        "C_values = [0.1, 1, 10]\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "for i, C in enumerate(C_values):\n",
        "    svm = SVC(C=C, kernel='linear')\n",
        "    svm.fit(X, y)\n",
        "\n",
        "    xx, yy = np.meshgrid(np.linspace(X[:, 0].min()-1, X[:, 0].max()+1, 50),\n",
        "                         np.linspace(X[:, 1].min()-1, X[:, 1].max()+1, 50))\n",
        "    Z = svm.decision_function(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "\n",
        "    axes[i].contour(xx, yy, Z, levels=[-1, 0, 1], colors=['red', 'black', 'blue'], linestyles=['dashed', 'solid', 'dashed'])\n",
        "    axes[i].scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
        "    axes[i].set_title(f\"SVM Decision Boundary with C={C}\")\n",
        "\n",
        "plt.show()\n",
        "'''"
      ],
      "metadata": {
        "id": "c3EAwqEtFlxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Write a Python program to train a Bernoulli Na√Øve Bayes classifier for binary classification on a dataset with binary features."
      ],
      "metadata": {
        "id": "axkl0a9AFmdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "documents = [\"Spam email detected\", \"Not spam\", \"Amazing offer for you\", \"Your account has been compromised\"]\n",
        "labels = [1, 0, 1, 0]\n",
        "\n",
        "vectorizer = CountVectorizer(binary=True)\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "model = BernoulliNB()\n",
        "model.fit(X, labels)\n",
        "\n",
        "new_texts = [\"Click this link for free money\", \"Important update about your account\"]\n",
        "X_new = vectorizer.transform(new_texts)\n",
        "predictions = model.predict(X_new)\n",
        "\n",
        "print(predictions)  # Outputs: [1, 0] (Spam, Not spam)\n",
        "'''"
      ],
      "metadata": {
        "id": "zoLVWbNxFpyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Write a Python program to apply feature scaling before training an SVM model and compare results with unscaled data."
      ],
      "metadata": {
        "id": "0tjgCY9_FqI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = datasets.make_classification(n_samples=300, n_features=10, n_classes=2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Without scaling\n",
        "svm_unscaled = SVC(kernel='linear')\n",
        "svm_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = svm_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "svm_scaled = SVC(kernel='linear')\n",
        "svm_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = svm_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"SVM accuracy without scaling: {accuracy_unscaled:.2f}\")\n",
        "print(f\"SVM accuracy with scaling: {accuracy_scaled:.2f}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "9f-Mp1MTFvEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Write a Python program to train a Gaussian Na√Øve Bayes model and compare the predictions before and after Laplace Smoothing."
      ],
      "metadata": {
        "id": "XAmR-tG7Fvs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=300, n_features=5, n_classes=2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Without Laplace smoothing (default)\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "y_pred = nb.predict(X_test)\n",
        "accuracy_without_smoothing = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Applying Laplace smoothing by modifying priors\n",
        "nb_smoothed = GaussianNB(var_smoothing=1e-9)\n",
        "nb_smoothed.fit(X_train, y_train)\n",
        "y_pred_smoothed = nb_smoothed.predict(X_test)\n",
        "accuracy_with_smoothing = accuracy_score(y_test, y_pred_smoothed)\n",
        "\n",
        "print(f\"Gaussian Na√Øve Bayes accuracy without smoothing: {accuracy_without_smoothing:.2f}\")\n",
        "print(f\"Gaussian Na√Øve Bayes accuracy with smoothing: {accuracy_with_smoothing:.2f}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "cZHzyYiMF2It"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C, gamma, kernel)."
      ],
      "metadata": {
        "id": "CKg5nTdUF2hM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import datasets\n",
        "\n",
        "X, y = datasets.make_classification(n_samples=300, n_features=10, n_classes=2, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': ['scale', 'auto', 0.1, 1],\n",
        "    'kernel': ['linear', 'poly', 'rbf']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best accuracy: {grid_search.best_score_:.2f}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "uNDzO73TGEHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "32. Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and check it improve accuracy."
      ],
      "metadata": {
        "id": "6V-gkPQWGEk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=300, weights=[0.9, 0.1], n_features=10, n_classes=2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Without class weighting\n",
        "svm_unweighted = SVC(kernel='linear')\n",
        "svm_unweighted.fit(X_train, y_train)\n",
        "y_pred_unweighted = svm_unweighted.predict(X_test)\n",
        "\n",
        "# With class weighting\n",
        "svm_weighted = SVC(kernel='linear', class_weight='balanced')\n",
        "svm_weighted.fit(X_train, y_train)\n",
        "y_pred_weighted = svm_weighted.predict(X_test)\n",
        "\n",
        "print(f\"Accuracy without class weighting: {accuracy_score(y_test, y_pred_unweighted):.2f}\")\n",
        "print(f\"Accuracy with class weighting: {accuracy_score(y_test, y_pred_weighted):.2f}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "UKFomyHXGGFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. Write a Python program to implement a Na√Øve Bayes classifier for spam detection using email data."
      ],
      "metadata": {
        "id": "SRROH53aGI5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "emails = [\"Buy cheap meds now!\", \"Meeting scheduled for tomorrow\", \"Win a free iPhone!\", \"Your package has been delivered\"]\n",
        "labels = [1, 0, 1, 0]  # 1 = spam, 0 = not spam\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(emails)\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X, labels)\n",
        "\n",
        "new_emails = [\"Claim your lottery prize now\", \"Important meeting update\"]\n",
        "X_new = vectorizer.transform(new_emails)\n",
        "predictions = model.predict(X_new)\n",
        "\n",
        "print(f\"Predictions: {predictions}\")  # Outputs: [1, 0] meaning spam and not spam\n",
        "'''"
      ],
      "metadata": {
        "id": "EuhZE9vmGN73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "34. Write a Python program to train an SVM Classifier and a Na√Øve Bayes Classifier on the same dataset and compare their accuracy."
      ],
      "metadata": {
        "id": "33mqer_PGPnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=300, n_features=10, n_classes=2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm = SVC(kernel='linear')\n",
        "nb = GaussianNB()\n",
        "\n",
        "svm.fit(X_train, y_train)\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "svm_pred = svm.predict(X_test)\n",
        "nb_pred = nb.predict(X_test)\n",
        "\n",
        "print(f\"SVM Accuracy: {accuracy_score(y_test, svm_pred):.2f}\")\n",
        "print(f\"Na√Øve Bayes Accuracy: {accuracy_score(y_test, nb_pred):.2f}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "Garlxyx8GT6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "35. Write a Python program to perform feature selection before training a Na√Øve Bayes classifier and compare results.\n"
      ],
      "metadata": {
        "id": "dK_sbC5gGURh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=300, n_features=20, n_classes=2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature selection\n",
        "selector = SelectKBest(chi2, k=10)\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "# Train Na√Øve Bayes on original and selected features\n",
        "nb_original = GaussianNB()\n",
        "nb_selected = GaussianNB()\n",
        "\n",
        "nb_original.fit(X_train, y_train)\n",
        "nb_selected.fit(X_train_selected, y_train)\n",
        "\n",
        "y_pred_original = nb_original.predict(X_test)\n",
        "y_pred_selected = nb_selected.predict(X_test_selected)\n",
        "\n",
        "print(f\"Na√Øve Bayes Accuracy with all features: {accuracy_score(y_test, y_pred_original):.2f}\")\n",
        "print(f\"Na√Øve Bayes Accuracy with selected features: {accuracy_score(y_test, y_pred_selected):.2f}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "XvcTVf-tGaPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "36. Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO) strategies on the Wine dataset and compare their accuracy."
      ],
      "metadata": {
        "id": "dRM0TllNGasu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# One-vs-Rest (OvR) Strategy\n",
        "svm_ovr = SVC(kernel='linear', decision_function_shape='ovr')\n",
        "svm_ovr.fit(X_train, y_train)\n",
        "y_pred_ovr = svm_ovr.predict(X_test)\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "\n",
        "# One-vs-One (OvO) Strategy\n",
        "svm_ovo = SVC(kernel='linear', decision_function_shape='ovo')\n",
        "svm_ovo.fit(X_train, y_train)\n",
        "y_pred_ovo = svm_ovo.predict(X_test)\n",
        "accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "\n",
        "print(f\"SVM Accuracy with One-vs-Rest (OvR): {accuracy_ovr:.2f}\")\n",
        "print(f\"SVM Accuracy with One-vs-One (OvO): {accuracy_ovo:.2f}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "Gake-8jgGgX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "37. Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast Cancer dataset and compare their accuracy."
      ],
      "metadata": {
        "id": "tQMfKVBUGg5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "kernels = ['linear', 'poly', 'rbf']\n",
        "\n",
        "for kernel in kernels:\n",
        "    svm = SVC(kernel=kernel)\n",
        "    svm.fit(X_train, y_train)\n",
        "    y_pred = svm.predict(X_test)\n",
        "    print(f\"SVM Accuracy with {kernel} kernel: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "6b7nhz78GnaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "38. Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the average accuracy."
      ],
      "metadata": {
        "id": "OA5h3Vn5Gn0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "X, y = make_classification(n_samples=300, n_features=10, n_classes=2, random_state=42)\n",
        "\n",
        "svm = SVC(kernel='linear')\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "scores = cross_val_score(svm, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "print(f\"Stratified K-Fold Cross-Validation Accuracy Scores: {scores}\")\n",
        "print(f\"Average Accuracy: {scores.mean():.2f}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "bpvig5nOGuVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "39. Write a Python program to train a Na√Øve Bayes classifier using different prior probabilities and compare performance."
      ],
      "metadata": {
        "id": "7x21JguZGuwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=300, n_features=5, n_classes=2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Default priors\n",
        "nb_default = GaussianNB()\n",
        "nb_default.fit(X_train, y_train)\n",
        "y_pred_default = nb_default.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "# Custom priors\n",
        "nb_custom = GaussianNB(priors=[0.7, 0.3])\n",
        "nb_custom.fit(X_train, y_train)\n",
        "y_pred_custom = nb_custom.predict(X_test)\n",
        "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
        "\n",
        "print(f\"Accuracy with default priors: {accuracy_default:.2f}\")\n",
        "print(f\"Accuracy with custom priors: {accuracy_custom:.2f}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "qzmqPtOEGyPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "40. Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and compare accuracy."
      ],
      "metadata": {
        "id": "BJkfTGodGyoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(n_samples=300, n_features=10, n_classes=2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature selection using RFE\n",
        "svm = SVC(kernel='linear')\n",
        "rfe = RFE(svm, n_features_to_select=5)\n",
        "X_train_selected = rfe.fit_transform(X_train, y_train)\n",
        "X_test_selected = rfe.transform(X_test)\n",
        "\n",
        "# Train SVM on selected features\n",
        "svm.fit(X_train_selected, y_train)\n",
        "y_pred = svm.predict(X_test_selected)\n",
        "\n",
        "print(f\"SVM Accuracy after RFE: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "jCcjW5-aG2lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "41. Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and F1-Score instead of accuracy."
      ],
      "metadata": {
        "id": "BR_WYyWdG29Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X, y = make_classification(n_samples=300, n_features=5, n_classes=2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svm.predict(X_test)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "'''"
      ],
      "metadata": {
        "id": "k1Q1c0FLG7bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "42. Write a Python program to train a Na√Øve Bayes Classifier and evaluate its performance using Log Loss (Cross-Entropy Loss)."
      ],
      "metadata": {
        "id": "6Iz1CRsrG71V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "X, y = make_classification(n_samples=300, n_features=5, n_classes=2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "y_pred_proba = nb.predict_proba(X_test)\n",
        "print(f\"Log Loss: {log_loss(y_test, y_pred_proba):.4f}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "H-nUmAI1HAeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "43. Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn."
      ],
      "metadata": {
        "id": "Uz9MCgy1HA29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "X, y = make_classification(n_samples=300, n_features=5, n_classes=2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='coolwarm')\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix of SVM Classifier\")\n",
        "plt.show()\n",
        "'''"
      ],
      "metadata": {
        "id": "dV8tAJAoHHDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "44. Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute Error (MAE) instead of MSE."
      ],
      "metadata": {
        "id": "ilwMKv5tHHgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "X, y = make_regression(n_samples=300, n_features=5, noise=0.1, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svr = SVR(kernel='rbf')\n",
        "svr.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svr.predict(X_test)\n",
        "print(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_pred):.2f}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "lo0Pq8M4HK82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "45. Write a Python program to train a Na√Øve Bayes classifier and evaluate its performance using the ROC-AUC score."
      ],
      "metadata": {
        "id": "exHRSwr-HLlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "X, y = make_classification(n_samples=300, n_features=5, n_classes=2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "y_pred_proba = nb.predict_proba(X_test)[:, 1]\n",
        "print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba):.2f}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "MVG-z3QLHOit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "46. Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve."
      ],
      "metadata": {
        "id": "LE68xg6JHO6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "X, y = make_classification(n_samples=300, n_features=5, n_classes=2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm = SVC(kernel='linear', probability=True)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "y_pred_proba = svm.predict_proba(X_test)[:, 1]\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "\n",
        "plt.plot(recall, precision, marker='.')\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve for SVM Classifier\")\n",
        "plt.show()\n",
        "  '''"
      ],
      "metadata": {
        "id": "rJ1nskLXHSYc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}